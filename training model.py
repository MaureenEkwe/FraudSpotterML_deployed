# -*- coding: utf-8 -*-
"""FraudSpotter_LR (training.py)

Automatically generated by Colab.

# **FraudSpotter: Job Posting Detection Using  Cassandra /ML Models**

Created By: Maureen Ekwebelem & YaeJin(Sally) Kang
"""

#importing relevant libraries

# Data handling and utilities
import pandas as pd
import numpy as np
import re

# Text processing
from sklearn.feature_extraction.text import TfidfVectorizer

# Preprocessing and sampling
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler


# Modeling
from sklearn.linear_model import LogisticRegression

# Evaluation metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Sparse matrix tools
from scipy.sparse import hstack, csr_matrix

import joblib

# Import file
df = pd.read_csv("/content/fake_job_postings.csv", engine='python', on_bad_lines='skip')

# View the first 5 rows
#df.head()

#print("Initial shape:", df.shape)

# Data Cleaning

# Drop >80% missing columns
missing = df.isna().mean() * 100
df.drop(columns=missing[missing > 80].index, inplace=True)

# Drop duplicates

df.info()

#Data Cleaning for ML model

# Replace all NaN with empty strings
text_columns = [
    'title', 'location', 'department', 'company_profile',
    'description', 'requirements', 'benefits',
    'employment_type', 'required_experience',
    'required_education', 'industry', 'function'
]

for col in text_columns:
    if col in df.columns:
        df[col] = df[col].fillna('')

# Combine text based columns into one big text column
df['text'] = df[text_columns].astype(str).agg(' '.join, axis=1)

def cleaned_text_ml(text):
    if not isinstance(text, str):
        return ""
    text = text.lower() # converts to lowercase
    text = re.sub(r'<[^>]+>', '', text) # remove HTML or XML tags
    text = re.sub(r'http\S+|www\S+', '', text)  # remove URLs
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = re.sub(r'\s+', ' ', text).strip() # tidy up spacing
    return text

df['text'] = df[text_columns].astype(str).agg(' '.join, axis=1)
df['text_clean'] = df['text'].apply(cleaned_text_ml)
#print(df.shape)

print(df['fraudulent'].isna().sum())
df['fraudulent'] = pd.to_numeric(df['fraudulent'], errors='coerce')
df = df.dropna(subset=['fraudulent'])
df['fraudulent'] = df['fraudulent'].astype(int)

#Train Test split
X = df['text_clean']
y = df['fraudulent']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# TF-IDF Vectorizer

tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# Balance (after combinining data)
ros = RandomOverSampler(random_state=42)
X_train_bal, y_train_bal = ros.fit_resample(X_train_tfidf, y_train)

# Logistic Regression Model
log_reg = LogisticRegression(
    solver='saga',
    max_iter=300,
    class_weight='balanced',
    random_state=42
)

log_reg.fit(X_train_bal, y_train_bal)

# Predictions
y_pred = log_reg.predict(X_test_tfidf)
y_proba = log_reg.predict_proba(X_test_tfidf)[:, 1]

# Save pickle files (artifacts)
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')
joblib.dump(log_reg, 'fraud_spotter.pkl')

print("\nSaved: tfidf_vectorizer.pkl, fraud_spotter.pkl")

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Classification Report (Precision, Recall, F1)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

#model training finished
#vectorizer+model saved as pickle files
